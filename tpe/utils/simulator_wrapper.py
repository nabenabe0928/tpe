""" A collection of utilities for a simulation of multi-fidelity optimizations.

In multi-fidelity optimizations, as we often evaluate configs in parallel,
it is essential to manage the order of config allocations to each worker.
We allow such appropriate allocations with the utilities in this file.
To guarantee the safety, we share variables with each process using the file system.

We first define some terminologies:
* simulator: A system that manages simulated runtimes to allow optimizations of a tabular/surrogate benchmark
             without waiting for actual runtimes.
* worker: Worker instantiates an objective function with a given config and evaluates it.
* proc: Each worker will be instantiated in each process.
* worker_id: The time hash generated for each worker.
* pid or proc_id: The process ID for each proc obtained by `os.getpid()`, which is an integer.
* index: The index of each worker. (it will be one of the integers in [0, N - 1] where N is the # of workers.)

Now we describe each file shared with each process.
Note that each file takes the json-dict format and we write down as follows:
    * key1 -- val1
    * key2 -- val2
    :
    * keyN -- valN

1. simulator_info/*/proc_alloc.json
    * proc1_id -- worker1_id
    * proc2_id -- worker2_id
    :
    * procN_id -- workerN_id
Note that we need this file only if we have multiple processes in one run.
For example, when we use multiprocessing, we might need it.
`procX_id` is fetched by `os.getpid()` and `workerX_id` is initially generated by `generate_time_hash()`.

2. simulator_info/*/results.json
    * loss -- List[loss at the n-th evaluation]
    * cumtime -- List[cumtime up to the n-th evaluation]
    * index -- List[the index of the worker of the n-th evaluation]
This file is necessary for post-hoc analysis.

3. simulator_info/*/runtime_cache.json
    * config1 -- List[runtimes]
    * config2 -- List[runtimes]
    :
    * configN -- List[runtimes]
This file tells you how long it took to evaluate configX up to the intermediate result.
Since we would like to use this information only for the restart of trainings,
we discard the information after each config reaches the full-budget training.
Each list gets more than two elements if evaluations of the same configs happen.

4. simulator_info/*/simulated_cumtime.json
    * worker1_id -- cumtime1
    * worker2_id -- cumtime2
    :
    * workerN_id -- cumtimeN
This file tells you how much time each worker virtually spends in the simulation
and we need this information to manage the order of job allocations to each worker.
"""
import fcntl
import hashlib
import os
import time
from _io import TextIOWrapper
from multiprocessing import Pool
from typing import Any, Callable, Dict, List, Protocol

import numpy as np

import ujson as json


DIR_NAME = "simulator_info/"
WORKER_CUMTIME_FILE_NAME = "simulated_cumtime.json"
RESULT_FILE_NAME = "results.json"
PROC_ALLOC_NAME = "proc_alloc.json"
RUNTIME_CACHE_FILE_NAME = "runtime_cache.json"
INF = 1 << 40


def generate_time_hash() -> str:
    hash = hashlib.sha1()
    hash.update(str(time.time()).encode("utf-8"))
    return hash.hexdigest()


def secure_read(func: Callable) -> Callable:
    def _inner(path: str, waiting_time: float = 1e-4, **kwargs):
        start = time.time()
        waiting_time *= 1 + np.random.random()
        fetched, output = False, None
        while not fetched:
            with open(path, "r") as f:
                try:
                    fcntl.flock(f, fcntl.LOCK_SH | fcntl.LOCK_NB)
                    output = func(f, **kwargs)
                    fetched = True
                except IOError:
                    time.sleep(waiting_time)
                    if time.time() - start >= 10:
                        raise TimeoutError("Timeout during secure read. Try again.")

        return output

    return _inner


def secure_edit(func: Callable) -> Callable:
    def _inner(path: str, waiting_time: float = 1e-4, **kwargs):
        start = time.time()
        waiting_time *= 1 + np.random.random()
        fetched, output = False, None
        while not fetched:
            with open(path, "r+") as f:
                try:
                    fcntl.flock(f, fcntl.LOCK_EX)
                    output = func(f, **kwargs)
                    f.truncate()
                    fetched = True
                except IOError:
                    time.sleep(waiting_time)
                    if time.time() - start >= 10:
                        raise TimeoutError("Timeout during secure read. Try again.")
                finally:
                    fcntl.flock(f, fcntl.LOCK_UN)

        return output

    return _inner


def init_simulator(dir_name: str) -> None:
    for fn in [WORKER_CUMTIME_FILE_NAME, RESULT_FILE_NAME, RUNTIME_CACHE_FILE_NAME, PROC_ALLOC_NAME]:
        path = os.path.join(dir_name, fn)
        with open(path, "a+") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            f.seek(0)
            content = f.read()
            if len(content) < 2:
                f.seek(0)
                f.truncate()
                f.write("{}")

            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


@secure_edit
def allocate_proc_to_worker(f: TextIOWrapper, pid: int) -> None:
    cur_alloc = json.load(f)
    cur_alloc[pid] = 0
    f.seek(0)
    json.dump(cur_alloc, f, indent=4)


@secure_edit
def complete_proc_allocation(f: TextIOWrapper) -> Dict[int, int]:
    alloc = json.load(f)
    sorted_pids = np.sort([int(pid) for pid in alloc.keys()])
    alloc = {pid: idx for idx, pid in enumerate(sorted_pids)}
    f.seek(0)
    json.dump(alloc, f, indent=4)
    return alloc


@secure_edit
def record_cumtime(f: TextIOWrapper, worker_id: str, runtime: float) -> float:
    record = json.load(f)
    cumtime = record.get(worker_id, 0.0) + runtime
    record[worker_id] = cumtime
    f.seek(0)
    json.dump(record, f, indent=4)
    return cumtime


@secure_edit
def cache_runtime(f: TextIOWrapper, config_key: str, runtime: float, update: bool = True) -> None:
    cache = json.load(f)
    if config_key not in cache:
        cache[config_key] = [runtime]
    elif update:
        cache[config_key][0] = runtime
    else:
        cache[config_key].append(runtime)

    cache[config_key] = np.sort(cache[config_key]).tolist()
    f.seek(0)
    json.dump(cache, f, indent=4)


@secure_edit
def delete_runtime(f: TextIOWrapper, config_key: str, index: float) -> None:
    cache = json.load(f)
    n_configs = len(cache.get(config_key, [])) > 0
    if n_configs <= 1:
        cache[config_key] = [0.0]  # we need to have at least one element.
    else:
        cache[config_key].pop(index)

    f.seek(0)
    json.dump(cache, f, indent=4)


@secure_read
def fetch_cache_runtime(f: TextIOWrapper) -> None:
    return json.load(f)


@secure_edit
def record_result(f: TextIOWrapper, results: Dict[str, float]) -> None:
    record = json.load(f)
    for key, val in results.items():
        if key not in record:
            record[key] = [val]
        else:
            record[key].append(val)

    f.seek(0)
    json.dump(record, f, indent=4)


@secure_read
def is_simulator_terminated(f: TextIOWrapper, max_evals: int) -> bool:
    return len(json.load(f)["loss"]) >= max_evals


@secure_read
def is_simulator_ready(f: TextIOWrapper, n_workers: int) -> bool:
    return len(json.load(f)) == n_workers


@secure_read
def is_allocation_ready(f: TextIOWrapper, n_workers: int) -> bool:
    return len(json.load(f)) == n_workers


@secure_read
def get_worker_id_to_idx(f: TextIOWrapper) -> Dict[str, int]:
    return {worker_id: idx for idx, worker_id in enumerate(json.load(f).keys())}


@secure_read
def is_min_cumtime(f: TextIOWrapper, worker_id: str) -> bool:
    cumtimes = json.load(f)
    proc_cumtime = cumtimes[worker_id]
    return min(cumtime for cumtime in cumtimes.values()) == proc_cumtime


def wait_proc_allocation(path: str, n_workers: int, waiting_time: float = 1e-2) -> Dict[int, int]:
    start = time.time()
    waiting_time *= 1 + np.random.random()
    while not is_allocation_ready(path, n_workers=n_workers):
        time.sleep(waiting_time)
        if time.time() - start >= n_workers * 5:
            raise TimeoutError("Timeout in the allocation of procs. Please make sure n_workers is correct.")

    return complete_proc_allocation(path)


def wait_all_workers(path: str, n_workers: int, waiting_time: float = 1e-2) -> Dict[str, int]:
    start = time.time()
    waiting_time *= 1 + np.random.random()
    while not is_simulator_ready(path, n_workers=n_workers):
        time.sleep(waiting_time)
        if time.time() - start >= n_workers * 5:
            raise TimeoutError("Timeout in creating a simulator. Please make sure n_workers is correct.")

    return get_worker_id_to_idx(path)


def wait_until_next(path: str, worker_id: str, waiting_time: float = 1e-4) -> None:
    waiting_time *= 1 + np.random.random()
    while not is_min_cumtime(path, worker_id=worker_id):
        time.sleep(waiting_time)


class ObjectiveFunc(Protocol):
    def __call__(self, eval_config: Dict[str, Any], budget: int) -> Dict[str, float]:
        raise NotImplementedError


class WorkerFunc:
    def __init__(
        self,
        subdir_name: str,
        n_workers: int,
        func: ObjectiveFunc,
        max_budget: int,
        loss_key: str = "loss",
        runtime_key: str = "runtime",
    ):
        worker_id = generate_time_hash()
        self._dir_name = os.path.join(DIR_NAME, subdir_name)
        os.makedirs(self.dir_name, exist_ok=True)
        init_simulator(dir_name=self.dir_name)
        self._cumtime_path = os.path.join(self.dir_name, WORKER_CUMTIME_FILE_NAME)
        record_cumtime(path=self._cumtime_path, worker_id=worker_id, runtime=0.0)

        self._func = func
        self._result_path = os.path.join(self._dir_name, RESULT_FILE_NAME)
        self._max_budget = max_budget
        self._runtime_key = runtime_key
        self._loss_key = loss_key
        self._terminated = False
        self._worker_id = worker_id
        self._worker_id_to_index = wait_all_workers(path=self._cumtime_path, n_workers=n_workers)
        time.sleep(1e-2)  # buffer before the optimization
        self._index = self._worker_id_to_index[self._worker_id]
        self._prev_timestamp = time.time()

    def __repr__(self) -> str:
        return f"Worker-{self._worker_id}"

    @property
    def dir_name(self) -> str:
        return self._dir_name

    def _get_cached_runtime_index(self, cached_runtimes: List[float], config_key: str, runtime: float) -> int:
        # a[i-1] < v <= a[i]: np.searchsorted(..., side="left")
        idx = np.searchsorted(cached_runtimes, runtime, side="left")
        return max(0, idx - 1)

    def _proc_output(self, eval_config: Dict[str, Any], budget: int) -> Dict[str, float]:
        output = self._func(eval_config, budget)
        config_key = str(eval_config)
        loss, total_runtime = output[self._loss_key], output[self._runtime_key]
        _path = os.path.join(self.dir_name, RUNTIME_CACHE_FILE_NAME)
        cached_runtimes = fetch_cache_runtime(_path).get(config_key, [0.0])
        cached_runtime_index = self._get_cached_runtime_index(cached_runtimes, config_key, total_runtime)
        cached_runtime = cached_runtimes[cached_runtime_index]

        actual_runtime = max(0.0, total_runtime - cached_runtime)
        # Start from the intermediate result, and hence we overwrite the cached runtime
        overwrite_min_runtime = cached_runtime < total_runtime
        if budget != self._max_budget:  # update the cache data
            cache_runtime(_path, config_key=config_key, runtime=total_runtime, update=overwrite_min_runtime)
        else:
            delete_runtime(_path, config_key=config_key, index=cached_runtime_index)

        return {self._loss_key: loss, self._runtime_key: actual_runtime}

    def __call__(self, eval_config: Dict[str, Any], budget: int) -> Dict[str, float]:
        if self._terminated:
            return {self._loss_key: INF, self._runtime_key: INF}

        sampling_time = time.time() - self._prev_timestamp
        output = self._proc_output(eval_config, budget)
        loss, runtime = output[self._loss_key], output[self._runtime_key]
        cumtime = record_cumtime(path=self._cumtime_path, worker_id=self._worker_id, runtime=runtime+sampling_time)
        wait_until_next(path=self._cumtime_path, worker_id=self._worker_id)
        self._prev_timestamp = time.time()
        row = dict(loss=loss, cumtime=cumtime, index=self._index)
        record_result(self._result_path, results=row)
        return output

    def finish(self) -> None:
        record_cumtime(path=self._cumtime_path, worker_id=self._worker_id, runtime=INF)
        self._terminated = True


class CentralWorker:
    def __init__(
        self,
        obj_func: ObjectiveFunc,
        n_workers: int,
        max_budget: int,
        max_evals: int,
        subdir_name: str,
        loss_key: str = "loss",
        runtime_key: str = "runtime",
    ):
        kwargs = dict(
            func=obj_func,
            n_workers=n_workers,
            subdir_name=subdir_name,
            max_budget=max_budget,
            loss_key=loss_key,
            runtime_key=runtime_key,
        )
        pool = Pool()
        results = []
        for _ in range(n_workers):
            results.append(pool.apply_async(WorkerFunc, kwds=kwargs))

        pool.close()
        pool.join()
        self._loss_key = loss_key
        self._runtime_key = runtime_key
        self._max_evals = max_evals
        self._workers = [result.get() for result in results]
        self._dir_name = self._workers[0].dir_name
        self._result_path = os.path.join(self._dir_name, RESULT_FILE_NAME)
        self._n_workers = n_workers
        self._pid_to_index: Dict[int, int] = {}

    def _init_alloc(self, pid: int) -> None:
        _path = os.path.join(self._dir_name, PROC_ALLOC_NAME)
        allocate_proc_to_worker(path=_path, pid=pid)
        self._pid_to_index = wait_proc_allocation(path=_path, n_workers=self._n_workers)

    def __call__(self, eval_config: Dict[str, Any], budget: int) -> Dict:
        pid = os.getpid()
        if len(self._pid_to_index) != self._n_workers:
            self._init_alloc(pid)

        worker_index = self._pid_to_index[pid]
        output = self._workers[worker_index](eval_config, budget)
        if is_simulator_terminated(self._result_path, max_evals=self._max_evals):
            self._workers[worker_index].finish()

        return output

""" A collection of utilities for a simulation of multi-fidelity optimizations.

In multi-fidelity optimizations, as we often evaluate configs in parallel,
it is essential to manage the order of config allocations to each worker.
We allow such appropriate allocations with the utilities in this file.
To guarantee the safety, we share variables with each process using the file system.

We first define some terminologies:
* simulator: A system that manages simulated runtimes to allow optimizations of a tabular/surrogate benchmark
             without waiting for actual runtimes.
* worker: Worker instantiates an objective function with a given config and evaluates it.
* proc: Each worker will be instantiated in each process.
* worker_id: The time hash generated for each worker.
* pid or proc_id: The process ID for each proc obtained by `os.getpid()`, which is an integer.
* index: The index of each worker. (it will be one of the integers in [0, N - 1] where N is the # of workers.)

Now we describe each file shared with each process.
Note that each file takes the json-dict format and we write down as follows:
    * key1 -- val1
    * key2 -- val2
    :
    * keyN -- valN

1. simulator_info/*/proc_alloc.json
    * proc1_id -- worker1_id
    * proc2_id -- worker2_id
    :
    * procN_id -- workerN_id
Note that we need this file only if we have multiple processes in one run.
For example, when we use multiprocessing, we might need it.
`procX_id` is fetched by `os.getpid()` and `workerX_id` is initially generated by `generate_time_hash()`.

2. simulator_info/*/results.json
    * loss -- List[loss at the n-th evaluation]
    * cumtime -- List[cumtime up to the n-th evaluation]
    * index -- List[the index of the worker of the n-th evaluation]
This file is necessary for post-hoc analysis.

3. simulator_info/*/state_cache.json
    * config1 -- List[Tuple[runtime, cumtime, budget, seed]]
    * config2 -- List[Tuple[runtime, cumtime, budget, seed]]
    :
    * configN -- List[Tuple[runtime, cumtime, budget, seed]]
This file tells you the states of each config.
Runtime tells how long it took to evaluate configX up to the intermediate result.
Since we would like to use this information only for the restart of trainings,
we discard the information after each config reaches the full-budget training.
Each list gets more than two elements if evaluations of the same configs happen.

4. simulator_info/*/simulated_cumtime.json
    * worker1_id -- cumtime1
    * worker2_id -- cumtime2
    :
    * workerN_id -- cumtimeN
This file tells you how much time each worker virtually spends in the simulation
and we need this information to manage the order of job allocations to each worker.
"""
import fcntl
import hashlib
import os
import time
from _io import TextIOWrapper
from multiprocessing import Pool
from typing import Any, Callable, Dict, List, NewType, Optional, Protocol, Tuple

import numpy as np

import ujson as json

from tpe.utils.tabular_benchmarks import BaseBenchData


DIR_NAME = "simulator_info/"
WORKER_CUMTIME_FILE_NAME = "simulated_cumtime.json"
RESULT_FILE_NAME = "results.json"
PROC_ALLOC_NAME = "proc_alloc.json"
STATE_CACHE_FILE_NAME = "state_cache.json"
INF = 1 << 40
_RuntimeType = NewType("_RuntimeType", float)
_CumtimeType = NewType("_CumtimeType", float)
_BudgetType = NewType("_BudgetType", int)
_SeedType = NewType("_SeedType", Optional[int])
_StateType = Tuple[_RuntimeType, _CumtimeType, _BudgetType, _SeedType]
INIT_STATE: _StateType = [0.0, 0.0, 0, None]


def _generate_time_hash() -> str:
    hash = hashlib.sha1()
    hash.update(str(time.time()).encode("utf-8"))
    return hash.hexdigest()


def secure_read(func: Callable) -> Callable:
    def _inner(path: str, waiting_time: float = 1e-4, **kwargs) -> Any:
        start = time.time()
        waiting_time *= 1 + np.random.random()
        fetched, output = False, None
        while not fetched:
            with open(path, "r") as f:
                try:
                    fcntl.flock(f, fcntl.LOCK_SH | fcntl.LOCK_NB)
                    output = func(f, **kwargs)
                    fetched = True
                except IOError:
                    time.sleep(waiting_time)
                    if time.time() - start >= 10:
                        raise TimeoutError("Timeout during secure read. Try again.")

        return output

    return _inner


def secure_edit(func: Callable) -> Callable:
    def _inner(path: str, waiting_time: float = 1e-4, **kwargs) -> Any:
        start = time.time()
        waiting_time *= 1 + np.random.random()
        fetched, output = False, None
        while not fetched:
            with open(path, "r+") as f:
                try:
                    fcntl.flock(f, fcntl.LOCK_EX)
                    output = func(f, **kwargs)
                    f.truncate()
                    fetched = True
                except IOError:
                    time.sleep(waiting_time)
                    if time.time() - start >= 10:
                        raise TimeoutError("Timeout during secure edit. Try again.")
                finally:
                    fcntl.flock(f, fcntl.LOCK_UN)

        return output

    return _inner


def _init_simulator(dir_name: str) -> None:
    for fn in [WORKER_CUMTIME_FILE_NAME, RESULT_FILE_NAME, STATE_CACHE_FILE_NAME, PROC_ALLOC_NAME]:
        path = os.path.join(dir_name, fn)
        with open(path, "a+") as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            f.seek(0)
            content = f.read()
            if len(content) < 2:
                f.seek(0)
                f.truncate()
                f.write("{}")

            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


@secure_edit
def _allocate_proc_to_worker(f: TextIOWrapper, pid: int) -> None:
    cur_alloc = json.load(f)
    cur_alloc[pid] = 0
    f.seek(0)
    json.dump(cur_alloc, f, indent=4)


@secure_edit
def _complete_proc_allocation(f: TextIOWrapper) -> Dict[int, int]:
    alloc = json.load(f)
    sorted_pids = np.sort([int(pid) for pid in alloc.keys()])
    alloc = {pid: idx for idx, pid in enumerate(sorted_pids)}
    f.seek(0)
    json.dump(alloc, f, indent=4)
    return alloc


@secure_edit
def _record_cumtime(f: TextIOWrapper, worker_id: str, cumtime: float) -> None:
    record = json.load(f)
    record[worker_id] = cumtime
    f.seek(0)
    json.dump(record, f, indent=4)


@secure_edit
def _cache_state(f: TextIOWrapper, config_hash: int, new_state: _StateType, update_index: Optional[int] = None) -> None:
    cache = {int(k): v for k, v in json.load(f).items()}
    if config_hash not in cache:
        cache[config_hash] = [new_state]
    elif update_index is not None:
        cache[config_hash][update_index] = new_state
    else:
        cache[config_hash].append(new_state)

    f.seek(0)
    json.dump(cache, f, indent=4)


@secure_edit
def _delete_state(f: TextIOWrapper, config_hash: int, index: int) -> None:
    cache = {int(k): v for k, v in json.load(f).items()}
    cache[config_hash].pop(index)
    if len(cache[config_hash]) == 0:
        cache.pop(config_hash)

    f.seek(0)
    json.dump(cache, f, indent=4)


@secure_read
def _fetch_cache_states(f: TextIOWrapper) -> Dict[int, List[_StateType]]:
    return {int(k): v for k, v in json.load(f).items()}


@secure_read
def _fetch_cumtimes(f: TextIOWrapper) -> Dict[str, float]:
    cumtimes = json.load(f)
    return cumtimes


@secure_edit
def _record_result(f: TextIOWrapper, results: Dict[str, float]) -> None:
    record = json.load(f)
    for key, val in results.items():
        if key not in record:
            record[key] = [val]
        else:
            record[key].append(val)

    f.seek(0)
    json.dump(record, f, indent=4)


@secure_read
def _is_simulator_terminated(f: TextIOWrapper, max_evals: int) -> bool:
    return len(json.load(f)["loss"]) >= max_evals


@secure_read
def _is_simulator_ready(f: TextIOWrapper, n_workers: int) -> bool:
    return len(json.load(f)) == n_workers


@secure_read
def _is_allocation_ready(f: TextIOWrapper, n_workers: int) -> bool:
    return len(json.load(f)) == n_workers


@secure_read
def _get_worker_id_to_idx(f: TextIOWrapper) -> Dict[str, int]:
    return {worker_id: idx for idx, worker_id in enumerate(json.load(f).keys())}


def _is_min_cumtime(path: str, worker_id: str) -> bool:
    cumtimes = _fetch_cumtimes(path=path)
    proc_cumtime = cumtimes[worker_id]
    return min(cumtime for cumtime in cumtimes.values()) == proc_cumtime


def _wait_proc_allocation(path: str, n_workers: int, waiting_time: float = 1e-2) -> Dict[int, int]:
    start = time.time()
    waiting_time *= 1 + np.random.random()
    while not _is_allocation_ready(path, n_workers=n_workers):
        time.sleep(waiting_time)
        if time.time() - start >= n_workers * 5:
            raise TimeoutError("Timeout in the allocation of procs. Please make sure n_workers is correct.")

    return _complete_proc_allocation(path)


def _wait_all_workers(path: str, n_workers: int, waiting_time: float = 1e-2) -> Dict[str, int]:
    start = time.time()
    waiting_time *= 1 + np.random.random()
    while not _is_simulator_ready(path, n_workers=n_workers):
        time.sleep(waiting_time)
        if time.time() - start >= n_workers * 5:
            raise TimeoutError("Timeout in creating a simulator. Please make sure n_workers is correct.")

    return _get_worker_id_to_idx(path)


def _wait_until_next(path: str, worker_id: str, waiting_time: float = 1e-4) -> None:
    waiting_time *= 1 + np.random.random()
    while not _is_min_cumtime(path, worker_id=worker_id):
        time.sleep(waiting_time)


class ObjectiveFunc(Protocol):
    def __call__(self, config: Dict[str, Any], budget: int, seed: Optional[int] = None) -> Dict[str, float]:
        raise NotImplementedError


class WorkerFunc:
    def __init__(
        self,
        subdir_name: str,
        n_workers: int,
        func: ObjectiveFunc,
        max_budget: int,
        loss_key: str = "loss",
        runtime_key: str = "runtime",
    ):
        worker_id = _generate_time_hash()
        self._dir_name = os.path.join(DIR_NAME, subdir_name)
        os.makedirs(self.dir_name, exist_ok=True)
        _init_simulator(dir_name=self.dir_name)
        self._cumtime_path = os.path.join(self.dir_name, WORKER_CUMTIME_FILE_NAME)
        _record_cumtime(path=self._cumtime_path, worker_id=worker_id, cumtime=0.0)

        self._rng = np.random.RandomState(42)
        self._func = func
        self._result_path = os.path.join(self._dir_name, RESULT_FILE_NAME)
        self._state_path = os.path.join(self.dir_name, STATE_CACHE_FILE_NAME)
        self._max_budget = max_budget
        self._runtime_key = runtime_key
        self._loss_key = loss_key
        self._terminated = False
        self._worker_id = worker_id
        self._worker_id_to_index = _wait_all_workers(path=self._cumtime_path, n_workers=n_workers)
        time.sleep(1e-2)  # buffer before the optimization
        self._index = self._worker_id_to_index[self._worker_id]
        self._prev_timestamp = time.time()
        self._cumtime = 0.0

    def __repr__(self) -> str:
        return f"Worker-{self._worker_id}"

    @property
    def dir_name(self) -> str:
        return self._dir_name

    def _get_cached_state_and_index(self, config_hash: int, budget: int) -> Tuple[_StateType, Optional[int]]:
        # _StateType = List[_RuntimeType, _CumtimeType, _BudgetType, _SeedType]
        cached_states = _fetch_cache_states(self._state_path).get(config_hash, [])[:]
        intermediate_avail = [state[1] <= self._cumtime and state[2] < budget for state in cached_states]
        cached_state_index = intermediate_avail.index(True) if any(intermediate_avail) else None
        if cached_state_index is None:
            init_state = INIT_STATE[:]
            # initial seed, note: 1 << 30 is a huge number that fits 32bit.
            init_state[-1] = self._rng.randint(1 << 30)
            return init_state, None
        else:
            return cached_states[cached_state_index][:], cached_state_index

    def _update_state(
        self,
        config_hash: int,
        budget: int,
        total_runtime: float,
        seed: Optional[int],
        cached_state_index: Optional[int]
    ) -> None:
        kwargs = dict(path=self._state_path, config_hash=config_hash)
        if budget != self._max_budget:  # update the cache data
            new_state = [total_runtime, self._cumtime, budget, seed]
            _cache_state(new_state=new_state, update_index=cached_state_index, **kwargs)
        elif cached_state_index is not None:  # if None, newly start and train till the end, so no need to delete.
            _delete_state(index=cached_state_index, **kwargs)

    def _proc_output(
        self, eval_config: Dict[str, Any], budget: int, bench_data: Optional[BaseBenchData]
    ) -> Dict[str, float]:
        config_hash = hash(str(eval_config))
        kwargs = dict(config_hash=config_hash, budget=budget)
        cached_state, cached_state_index = self._get_cached_state_and_index(**kwargs)
        cached_runtime, _, _, seed = cached_state
        bench_data_kwargs = ({} if bench_data is None else dict(bench_data=bench_data))
        output = self._func(config=eval_config, budget=budget, seed=seed, **bench_data_kwargs)
        loss, total_runtime = output[self._loss_key], output[self._runtime_key]
        actual_runtime = max(0.0, total_runtime - cached_runtime)
        self._cumtime += actual_runtime
        self._update_state(total_runtime=total_runtime, cached_state_index=cached_state_index, seed=seed, **kwargs)
        return {self._loss_key: loss, self._runtime_key: actual_runtime}

    def __call__(
        self, eval_config: Dict[str, Any], budget: int, bench_data: Optional[BaseBenchData] = None
    ) -> Dict[str, float]:
        if self._terminated:
            return {self._loss_key: INF, self._runtime_key: INF}

        self._cumtime += time.time() - self._prev_timestamp  # sampling time
        output = self._proc_output(eval_config, budget, bench_data)
        _record_cumtime(path=self._cumtime_path, worker_id=self._worker_id, cumtime=self._cumtime)
        _wait_until_next(path=self._cumtime_path, worker_id=self._worker_id)
        self._prev_timestamp = time.time()
        row = dict(loss=output[self._loss_key], cumtime=self._cumtime, index=self._index)
        _record_result(self._result_path, results=row)
        return output

    def finish(self) -> None:
        _record_cumtime(path=self._cumtime_path, worker_id=self._worker_id, cumtime=INF)
        self._terminated = True


class CentralWorker:
    def __init__(
        self,
        obj_func: ObjectiveFunc,
        n_workers: int,
        max_budget: int,
        max_evals: int,
        subdir_name: str,
        loss_key: str = "loss",
        runtime_key: str = "runtime",
    ):
        worker_kwargs = dict(
            func=obj_func,
            n_workers=n_workers,
            subdir_name=subdir_name,
            max_budget=max_budget,
            loss_key=loss_key,
            runtime_key=runtime_key,
        )
        self._n_workers = n_workers
        self._workers: List[WorkerFunc]
        self._init_workers(worker_kwargs)

        self._max_evals = max_evals
        self._dir_name = self._workers[0].dir_name
        self._result_path = os.path.join(self._dir_name, RESULT_FILE_NAME)
        self._pid_to_index: Dict[int, int] = {}

    def _init_workers(self, worker_kwargs: Dict[str, Any]) -> None:
        pool = Pool()
        results = []
        for _ in range(self._n_workers):
            results.append(pool.apply_async(WorkerFunc, kwds=worker_kwargs))

        pool.close()
        pool.join()
        self._workers = [result.get() for result in results]

    def _init_alloc(self, pid: int) -> None:
        _path = os.path.join(self._dir_name, PROC_ALLOC_NAME)
        _allocate_proc_to_worker(path=_path, pid=pid)
        self._pid_to_index = _wait_proc_allocation(path=_path, n_workers=self._n_workers)

    def __call__(self, eval_config: Dict[str, Any], budget: int, bench_data: Optional[BaseBenchData] = None) -> Dict:
        pid = os.getpid()
        if len(self._pid_to_index) != self._n_workers:
            self._init_alloc(pid)

        worker_index = self._pid_to_index[pid]
        output = self._workers[worker_index](eval_config=eval_config, budget=budget, bench_data=bench_data)
        if _is_simulator_terminated(self._result_path, max_evals=self._max_evals):
            self._workers[worker_index].finish()

        return output
